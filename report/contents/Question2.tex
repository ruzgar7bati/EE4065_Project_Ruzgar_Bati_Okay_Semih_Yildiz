\section{Question 2: Handwritten Digit Detection using YOLO}

This task focuses on detecting and localizing handwritten digits using a YOLO-based object detection model. The objective is to train and evaluate a lightweight detector capable of recognizing the digits 0, 4, and 7, while considering the constraints of eventual deployment on an embedded platform.

\subsection{Problem Description}

The goal is to train an object detection model that can accurately locate and classify handwritten digits 0, 4, and 7 in grayscale images. The workflow includes dataset preparation, model training and validation, evaluation on a held-out test set, and an investigation of deployment feasibility on the ESP32-CAM.

\subsection{Method}

A YOLOv8-based object detection approach is used. To keep the model lightweight and suitable for embedded deployment, the task is intentionally limited to three digit classes. The overall workflow consists of the following steps:
\begin{enumerate}
    \item Dataset preparation with controlled data augmentation
    \item Model training and hyperparameter selection
    \item Evaluation on a separate test set
    \item Investigation of model conversion and deployment constraints
\end{enumerate}

\subsection{Python Implementation on PC}

All training and evaluation are performed using Python 3.11 and the Ultralytics YOLOv8 framework. The implementation logic and key code sections are shown in Appendix~\ref{appendix:q2}.

\subsubsection{Dataset Preparation}

Dataset preparation is performed using a Python script. The key functions are shown in Appendix~\ref{appendix:q2:dataset}. The preparation process carries out the following operations:
\begin{itemize}
    \item Splits the dataset into training, validation, and test sets based on image indices
    \item Applies controlled data augmentations, including noise, blur, small rotations (±8°), brightness, and contrast changes
    \item Assigns class labels based on file naming conventions (e.g., \texttt{0\_1.png} corresponds to digit 0)
    \item Automatically extracts bounding boxes around digits and adds 10\% padding
\end{itemize}

Data augmentation is used to increase robustness to variations in handwriting and image quality. Flipping and aggressive cropping are intentionally avoided to preserve digit readability.

\subsubsection{Model Training}

Model training is implemented as shown in Appendix~\ref{appendix:q2:training}. A YOLOv8n model is selected due to its small size and suitability for resource-constrained environments. The following training strategy is applied:
\begin{itemize}
    \item Learning rates of 0.001 and 0.01 are evaluated
    \item Batch sizes of 2, 4, and 8 are tested
    \item Training is performed for 50 epochs
    \item Model performance is monitored using validation mAP50
\end{itemize}

Training is conducted using the training set, hyperparameters are evaluated on the validation set, and the test set is used only for final evaluation. All images are resized to 320×320 pixels during training.

\subsubsection{Model Evaluation}

The trained models are evaluated using the evaluation logic shown in Appendix~\ref{appendix:q2:evaluation}. The evaluation computes standard object detection metrics, including mAP50, mAP50–95, precision, and recall. Results are saved to CSV files (\texttt{validation\_metrics.csv}, \texttt{test\_results.csv}) for analysis.

An additional inference script is used to generate qualitative results by drawing bounding boxes and confidence scores on test images.

\subsubsection{Hyperparameter Search Results}

Six combinations of learning rate and batch size are evaluated. Validation results are summarized in Table~\ref{tab:hyperparameter_results}. The configuration with a learning rate of 0.001 and a batch size of 8 achieves the best validation performance, with an mAP50 of 0.954. These results are recorded in the file \texttt{validation\_metrics.csv}.

\begin{table}[H]
    \centering
    \caption{Validation results for different learning rate and batch size combinations. Values are taken from \texttt{validation\_metrics.csv}.}
    \label{tab:hyperparameter_results}
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Name} & \textbf{LR} & \textbf{Batch} & \textbf{mAP50} & \textbf{mAP50-95} & \textbf{Precision} & \textbf{Recall} \\
        \midrule
        lr0.001\_batch2 & 0.001 & 2 & 0.834 & 0.769 & 0.738 & 0.940 \\
        lr0.001\_batch4 & 0.001 & 4 & 0.905 & 0.876 & 0.784 & 0.889 \\
        lr0.001\_batch8 & 0.001 & 8 & \textbf{0.954} & \textbf{0.954} & \textbf{0.813} & \textbf{0.822} \\
        lr0.01\_batch2  & 0.01  & 2 & 0.767 & 0.700 & 0.719 & 0.722 \\
        lr0.01\_batch4  & 0.01  & 4 & 0.905 & 0.876 & 0.784 & 0.889 \\
        lr0.01\_batch8  & 0.01  & 8 & 0.954 & 0.954 & 0.813 & 0.822 \\
        \bottomrule
    \end{tabular}
\end{table}

The best model configuration is recorded in \texttt{best\_model\_info.txt}, confirming that the selected model is \texttt{lr0.001\_batch8} with learning rate 0.001, batch size 8, and validation mAP50 of 0.9542. The trained weights are saved as \texttt{best.pt} in the corresponding experiment directory.

\subsubsection{Training Progress}

Figure~\ref{fig:q2_training_results} illustrates the training behavior over 50 epochs. The loss curves show a steady decrease, while precision, recall, and mAP metrics increase consistently. Both mAP50 and mAP50–95 reach high values, indicating successful convergence of the model.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/q2_training_results.png}
    \caption{Training curves for the best experiment (\texttt{lr0.001\_batch8}). Loss decreases steadily while precision, recall, and mAP metrics increase, showing stable learning over 50 epochs.}
    \label{fig:q2_training_results}
\end{figure}

\subsubsection{Test Set Detection Results}

The best-performing model is evaluated on the test set. Sample detection results are shown in Figure~\ref{fig:q2_test_detections}. The model successfully detects and localizes handwritten digits across different variations, including rotation, contrast changes, and noise.

Misclassification occurs primarily between digits 4 and 7. This behavior is expected, as these digits share similar structural elements in handwritten form, such as vertical strokes and diagonal lines.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/q2_result_0_6.png}
        \caption{Digit 0}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/q2_result_4_6_aug1_rotate.png}
        \caption{Digit 4 (rotated)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/q2_result_7_6_aug2_contrast.png}
        \caption{Digit 7 (contrast change)}
    \end{subfigure}
    \caption{Example detections from the test set using the best model. The detector correctly finds and labels digits 0, 4, and 7 under different augmentations.}
    \label{fig:q2_test_detections}
\end{figure}

To visualize the training data and labels, Figure~\ref{fig:q2_val_batch} shows a validation batch with ground truth boxes for all three digit classes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/q2_val_batch0_labels.jpg}
    \caption{Validation batch with ground truth labels for digits 0, 4, and 7. This view helps confirm that the dataset is annotated correctly.}
    \label{fig:q2_val_batch}
\end{figure}

\subsubsection{Evaluation Curves}

Model performance across different confidence thresholds is analyzed using precision–recall and F1–confidence curves. The precision–recall curves in Figure~\ref{fig:q2_pr_curve} show strong performance for all three classes, with an overall mAP@0.5 of 0.954.

The F1–confidence curves in Figure~\ref{fig:q2_f1_curve} indicate that the optimal confidence threshold for balanced precision and recall is approximately 0.29, yielding a maximum F1 score of 0.80. The digit 7 class shows a more pronounced drop in F1 at higher confidence thresholds, reflecting confusion with digit 4.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/q2_box_pr_curve.png}
    \caption{Precision–recall curves for each digit class. All three classes achieve high area under the curve, with overall mAP@0.5 = 0.954.}
    \label{fig:q2_pr_curve}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/q2_box_f1_curve.png}
    \caption{F1 score as a function of confidence threshold. The best trade-off between precision and recall occurs around a confidence of 0.29, where the overall F1 score is about 0.80. Digit 7 shows a stronger drop at high confidence, matching the observed confusion with digit 4.}
    \label{fig:q2_f1_curve}
\end{figure}

\subsubsection{Overfitting Strategy}

Training is intentionally performed for the full 50 epochs without early stopping, even after signs of overfitting appear. This decision is motivated by the planned deployment of the model on highly constrained hardware.

A fully trained model tends to learn stronger feature representations, which are more likely to survive aggressive compression and integer quantization. Since the primary objective is reliable digit detection rather than perfect classification, limited overfitting is considered acceptable for this application.

\subsubsection{Deployment Considerations}

Several attempts are made to convert the trained model for embedded deployment:
\begin{itemize}
    \item Conversion attempts using Python 3.10 and older YOLO versions
    \item Exploration of the PyTorch → ONNX → TensorFlow → TFLite → TFLite Micro pipeline
    \item Encountered compatibility issues related to library versions
\end{itemize}

The current implementation remains PC-based, with deployment to the ESP32-CAM identified as future work. The chosen training and overfitting strategy is expected to improve robustness after heavy quantization.

\subsection{Results and Evaluation}

The final model achieves strong detection performance for the selected digit classes. Digit 0 is detected most reliably, while digits 4 and 7 exhibit occasional confusion due to visual similarity. Overall, the results demonstrate that a lightweight YOLO-based model can effectively detect handwritten digits in controlled conditions, providing a strong foundation for future embedded deployment.

The best model configuration and its performance metrics are summarized in Table~\ref{tab:q2_test_results}. The validation metrics are taken from \texttt{best\_model\_info.txt} and \texttt{validation\_metrics.csv}, while test metrics are recorded in \texttt{test\_results.csv}.

\begin{table}[H]
    \centering
    \caption{Best model configuration and performance summary.}
    \label{tab:q2_test_results}
    \small
    \begin{tabular}{lcc}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Model name & lr0.001\_batch8 \\
        Learning rate & 0.001 \\
        Batch size & 8 \\
        Validation mAP50 & 0.9542 \\
        Validation Precision & 0.813 \\
        Validation Recall & 0.822 \\
        \bottomrule
    \end{tabular}
\end{table}

The test evaluation results are documented in \texttt{test\_results.csv}. The qualitative detections shown in Figure~\ref{fig:q2_test_detections} demonstrate that the model successfully detects and classifies digits on the held-out test set, with the strong validation scores from Table~\ref{tab:hyperparameter_results} confirming overall model performance.
